{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline ,Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from joblib import dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroups')\n",
    "# This code from http#s://www.kaggle.com/mansijharia with some edit\n",
    "# May take a few time \n",
    "\n",
    "texts = []\n",
    "labels_index = {}\n",
    "labels = []\n",
    "\n",
    "for name in sorted(os.listdir((BASE_DIR+'20_newsgroups'))):\n",
    "    path = os.path.join(BASE_DIR,'20_newsgroups', name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #Skip the matadata at 1st pragraph.\n",
    "                    i = t.find('\\n\\n')\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaing by RE \n",
    "for i in range(0,len(texts)):\n",
    "    texts[i] =texts[i].strip()\n",
    "    texts[i] =re.sub(r'\\=+',' ', texts[i])#To remove any == characters\n",
    "    texts[i] =re.sub(r'\\|+',' ', texts[i])#To remove any | characters\n",
    "    texts[i] =re.sub('[<>]', ' ',texts[i])#To remove < and > characters\n",
    "    texts[i] =re.sub(r'\\[\\]+',' ', texts[i])#To remove any [] characters\n",
    "    texts[i] =re.sub(r'\\(\\)+',' ', texts[i])#To remove any () empty parentheses\n",
    "    texts[i] =re.sub('--+', ' ',texts[i])#To remove multiple spaces \n",
    "    texts[i] =re.sub(\"[__]+\", ' ', texts[i])#To remove lines\n",
    "    texts[i] =re.sub('\\^+',' ', texts[i])#To remove ^ characters  \n",
    "    texts[i] =re.sub(r'[/*\\\\*/]',' ', texts[i])#To remove \\/ characters\n",
    "    texts[i] =re.sub('([\\w\\.-]+)@([\\w\\.-]+)',' ', texts[i])#To remove any emails \n",
    "    texts[i] =re.sub('[^a-zA-Z]',' ',texts[i])#To remove nunEnglish and digital characters\n",
    "    texts[i] =re.sub(r'\\~\\~+',' ', texts[i])#To remove any == characters\n",
    "    texts[i] =re.sub('\\n', ' ', texts[i])#To remove lines\n",
    "    texts[i] =re.sub('\\t', ' ',texts[i])#To remove tab spaces \n",
    "    texts[i] =re.sub('  +', ' ',texts[i])#To remove multiple spaces \n",
    "    # if i==12:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we need to reduce the number of words\n",
    "def size_word(texts):\n",
    "    short_words=[]\n",
    "    for j in range(0,len(texts)):\n",
    "        clean_tokens=tokanizer(texts[j])\n",
    "        texts[j] =[]              \n",
    "        for i in range(0,len(clean_tokens)):\n",
    "            if (len(clean_tokens[i])>=3 | len(clean_tokens[i])<=15):\n",
    "                texts[j].append(clean_tokens[i])\n",
    "                return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokanizer(texts):\n",
    "    clean_tokens=[]\n",
    "    Lemmatizer=WordNetLemmatizer()\n",
    "    stop_words= set(stopwords.words(\"english\"))\n",
    "    for i in range(0,len(texts)):\n",
    "        tokens = word_tokenize(texts[i])\n",
    "        for tokn in tokens:\n",
    "            clean_tok= Lemmatizer.lemmatize(tokn) \n",
    "            if clean_tok not in stop_words:\n",
    "                clean_tokens.append(clean_tok)\n",
    "    return clean_tokens        \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels,test_size=0.33)\n",
    "count_vect =  CountVectorizer()\n",
    "transformer =  TfidfTransformer()\n",
    "count_vect.fit(X_train)\n",
    "count_tran = count_vect.transform(X_train)\n",
    "transformer.fit(count_tran)\n",
    "tf_transform = transformer.transform(count_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.60      0.56       306\n",
      "           1       0.68      0.62      0.65       389\n",
      "           2       0.80      0.66      0.72       384\n",
      "           3       0.69      0.67      0.68       335\n",
      "           4       0.77      0.77      0.77       314\n",
      "           5       0.78      0.77      0.78       327\n",
      "           6       0.82      0.67      0.73       414\n",
      "           7       0.80      0.79      0.79       325\n",
      "           8       0.88      0.92      0.90       320\n",
      "           9       0.89      0.80      0.84       354\n",
      "          10       0.92      0.91      0.92       335\n",
      "          11       0.81      0.88      0.85       306\n",
      "          12       0.55      0.73      0.62       242\n",
      "          13       0.80      0.85      0.82       325\n",
      "          14       0.87      0.87      0.87       320\n",
      "          15       0.86      0.67      0.75       426\n",
      "          16       0.80      0.69      0.74       386\n",
      "          17       0.87      0.90      0.89       317\n",
      "          18       0.55      0.71      0.62       253\n",
      "          19       0.28      0.42      0.34       222\n",
      "\n",
      "    accuracy                           0.75      6600\n",
      "   macro avg       0.75      0.74      0.74      6600\n",
      "weighted avg       0.76      0.75      0.75      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(tf_transform,y_train)\n",
    "c_tran_test = count_vect.transform(X_test)\n",
    "y_pred = classifier.predict(transformer.transform(c_tran_test))\n",
    "print(classification_report(y_pred,y_test))\n",
    "# dump(classifier, '\\Group Project') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.62      0.46       197\n",
      "           1       0.31      0.90      0.47       123\n",
      "           2       0.43      0.94      0.59       147\n",
      "           3       0.36      0.95      0.53       126\n",
      "           4       0.98      0.09      0.16      3567\n",
      "           5       0.39      0.96      0.55       130\n",
      "           6       0.22      0.90      0.35        81\n",
      "           7       0.47      0.95      0.63       159\n",
      "           8       0.42      0.99      0.59       141\n",
      "           9       0.46      0.96      0.62       152\n",
      "          10       0.51      0.98      0.67       171\n",
      "          11       0.46      0.99      0.63       153\n",
      "          12       0.43      0.95      0.59       146\n",
      "          13       0.51      0.97      0.67       181\n",
      "          14       0.44      0.99      0.61       142\n",
      "          15       0.46      0.97      0.63       158\n",
      "          16       0.51      0.86      0.64       200\n",
      "          17       0.61      0.95      0.74       210\n",
      "          18       0.40      0.65      0.50       202\n",
      "          19       0.32      0.50      0.39       214\n",
      "\n",
      "    accuracy                           0.45      6600\n",
      "   macro avg       0.45      0.85      0.55      6600\n",
      "weighted avg       0.73      0.45      0.35      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 3\n",
    "weights = 'uniform'\n",
    "weights = 'distance'\n",
    "classifier =KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "classifier.fit(tf_transform,y_train)\n",
    "c_tran_test = count_vect.transform(X_test)\n",
    "y_pred = classifier.predict(transformer.transform(c_tran_test))\n",
    "print(classification_report(y_pred,y_test))\n",
    "# dump(classifier, '\\Group Project')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2633333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', ngram_range = (1,2), binary = True, sublinear_tf=True)), # CountVectorizer, TfidfVectorizer\n",
    "        ('clf', KNeighborsClassifier())])  # MultinomialNB, LogisticRegression, SGDClassifier\n",
    "\n",
    "# parameters = {\n",
    "#        'clf__n_neighbors': (5, 10, 100, 200),\n",
    "#        'clf__weights': ('uniform', 'distance')\n",
    "# }\n",
    "Bast_parameters = {\n",
    "       'clf__n_neighbors':(5,7) ,\n",
    "       'clf__weights': ('distance','uniform')\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, Bast_parameters)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.score(X_test, y_test))\n",
    "#16:43 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators = [\n",
    "#      ('svr', make_pipeline(StandardScaler(), LinearSVC(random_state=42))),\n",
    "#      ('rf', RandomForestClassifier(n_estimators=10, random_state=42))]\n",
    "# clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "# clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ac516125b3ee16d4eaa41f008fade7bad50b808bd1b6d74d2e8ae0015ba9066"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
