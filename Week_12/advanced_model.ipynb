{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced model\n",
    "Rather than basic Linear model, Ensemble, etc that uses sklearn, and rather than using basic DNN, we'll do something more complicated that other uses (though they're not necessarily current SOTA on both model and techniques). \n",
    "\n",
    "- Cleaning of data (With additional mapping all to American English from British English). \n",
    "\n",
    "## Cleaning Data\n",
    "Information on British --> American: Get a mapping from the internet, get all unique words from the whole corpus, get all the words that're British, and find their corresponding American, and apply it throughout whole corpus using regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/fastai2/Music/BNC_Corpus/download/Texts/news\")\n",
    "this = path/\"A1E.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bncDoc', {'{http://www.w3.org/XML/1998/namespace}id': 'A1E'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = ET.parse(this)\n",
    "root = tree.getroot()\n",
    "root.tag, root.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtext = list(root)[1]\n",
    "div1 = list(wtext)[0]\n",
    "head1 = list(div1)[0]\n",
    "s1 = list(head1)[0]\n",
    "w1 = list(s1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Latest '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's skip that for now and we'll see how it goes later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "all_xs, all_y = fetch_20newsgroups(subset=\"all\", remove=('headers', 'footers', 'quotes'),\n",
    "                    shuffle=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fastai2/notebooks/DataGlacier/NLP_GroupProject_DG/Week_12',\n",
       " '/home/fastai2/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/pythonFiles',\n",
       " '/home/fastai2/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/pythonFiles/lib/python',\n",
       " '/anaconda/envs/fastai/lib/python38.zip',\n",
       " '/anaconda/envs/fastai/lib/python3.8',\n",
       " '/anaconda/envs/fastai/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/anaconda/envs/fastai/lib/python3.8/site-packages',\n",
       " '/anaconda/envs/fastai/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg',\n",
       " '/anaconda/envs/fastai/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/fastai2/.ipython']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "parent_path = Path(\"/home/fastai2/notebooks/DataGlacier\")\n",
    "sys.path.append(str(parent_path/\"NLP_GroupProject_DG/python_files\"))\n",
    "\n",
    "from nlputils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish thresholding. \n",
      "Finish removing xxunk above threshold.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cb2b4625f94abf92094bef3919fe18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish cleaning data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11708"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 7\n",
    "\n",
    "choice = np.load(f\"choice_{k}.npy\")\n",
    "all_xs = np.array(all_xs)[threshold_subset(all_xs, k)]\n",
    "print(\"Finish thresholding. \")\n",
    "all_xs = all_xs[choice < 0.1]\n",
    "print(\"Finish removing xxunk above threshold.\")\n",
    "all_xs = np.array([clean_data(x, False) for x in tqdm(all_xs)])\n",
    "print(\"Finish cleaning data.\")\n",
    "len(all_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One is thinking of doing this manually. Thing is, one isn't even sure if we could deal with everything. There's no promise. \n",
    "\n",
    "And we don't have to do it for **everything**, just those that are confusing. WE could still mix british english and american english **as long as there're no confusion** (such as both being used). For a single word, if there is only in British English used all the time, just do that. \n",
    "\n",
    "**Weakness**: When you try to apply it to real product, you need to check again for any British English because the corpus **cannot contain all** British English. \n",
    "\n",
    "There are also some thing on non-english words. One is thinkingo f just leaving it there, because there seems to be some that contains these and we don't really know how to clean these things out without doing quite a lot of work. (brute force comparison with english dictionary, and then have to decide again things especially for science like SCSI that's a computer term but not in the English dict, etc that is complicated). So we'll just leave it there as some noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: counter = load_pickle(f\"counter_{k}.pkl\")\n",
    "except Exception: \n",
    "    counter = Counter()\n",
    "    for data in tqdm(all_xs): counter += Counter(data.split())\n",
    "    save_pickle(f\"counter_{k}.pkl\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower all cases\n",
    "- Remove all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(s): \n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# lower all cases\n",
    "our_vocab = {v.lower() for v in counter}\n",
    "del counter\n",
    "\n",
    "# remove all punctuations\n",
    "our_vocab = {strip_punc(v) for v in our_vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ignore some things like `endeavor` can only be used as verb but it's British English type can be used both as verb and noun thingy. Too complicated. \n",
    "\n",
    "Commonly we'll have these [spelling method](https://www.oxfordinternationalenglish.com/differences-in-british-and-american-spelling/) differences and we also have [common words](https://www.thoughtco.com/american-english-to-british-english-4010264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discovery method\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def brit_to_amer(ending, brit_rule, americ_rule):\n",
    "    r = re.compile(ending)\n",
    "    compare_list = list(filter(r.match, our_vocab))\n",
    "    ame_list = [re.sub(brit_rule, americ_rule, s) for s in compare_list]\n",
    "\n",
    "    non_empty = {}\n",
    "    for b, a in tqdm(zip(compare_list, ame_list)):\n",
    "        m = re.compile(rf'{a}')\n",
    "        z = list(filter(m.match, our_vocab))\n",
    "        if a in z: non_empty[b] = a\n",
    "\n",
    "    return non_empty\n",
    "\n",
    "\n",
    "british_eng = {\n",
    "    \"rumour\": \"rumor\",\n",
    "    \"vapour\": \"vapor\",\n",
    "    \"arbour\": \"arbor\",\n",
    "    \"colour\": \"color\",\n",
    "    \"behaviour\": \"behavior\",\n",
    "    \"saviour\": \"savior\",\n",
    "    \"favour\": \"favor\",\n",
    "    \"armour\": \"armor\",\n",
    "    \"honour\": \"honor\",\n",
    "    \"inferiour\": \"inferior\",\n",
    "    \"labour\": \"labor\",\n",
    "    \"humour\": \"humor\",\n",
    "    \"endeavour\": \"endeavor\",\n",
    "    \"harbour\": \"harbor\",\n",
    "    \"fervour\": \"fervor\",\n",
    "    \"parlour\": \"parlor\",\n",
    "    \"neighbour\": \"neighbor\",\n",
    "    \"flavour\": \"flavor\",\n",
    "    \"belabour\": \"belabor\",\n",
    "    'survivour': 'survivor',  # end of our --> or.\n",
    "    'aerial': \"antenna\",\n",
    "    'anywhere': 'anyplace', \n",
    "    # 'autumn': 'fall'   # which fall we're deciding? fall or fall?\n",
    "    \"solicitor\": \"attorney\",\n",
    "    'biscuit': 'cookie',\n",
    "    'bonnet': 'hood',\n",
    "    'janitor': 'aretaker',\n",
    "    'constable': 'patrolman',\n",
    "    'dynamo': 'generator',\n",
    "    # and others it's just too many one decide to leave it here for now. \n",
    "}\n",
    "\n",
    "# our to or is too dirty to be used. \n",
    "british_eng.update(brit_to_amer(r'[a-z]+ise$', r'ise', r'ize')) # ise --> ize. \n",
    "british_eng.update(brit_to_amer(r'[a-z]+yse$', 'yse', 'yze'))  # use --> yze\n",
    "british_eng.update(brit_to_amer(r'[a-z]+ae[a-z]$', 'ae', 'e'))  # ae --> e\n",
    "del british_eng[\"michael\"], british_eng[\"laer\"], british_eng[\"caen\"]\n",
    "del british_eng[\"raes\"]\n",
    "# oe --> e checked nothing useful (mostly useful translated become rubbish)\n",
    "british_eng.update({'defence': 'defense',\n",
    " 'sence': 'sense',\n",
    " 'selfdefence': 'selfdefense',\n",
    " 'nonsence': 'nonsense',\n",
    " 'pretence': 'pretense',\n",
    " 'absence': 'absense',\n",
    " 'essence': 'essense',\n",
    " 'licence': 'license',\n",
    " 'offence': 'offense'})  # ence --> ense after deleting rubbish.\n",
    "british_eng.update({'catalogue': 'catalog'})  # ogue --> og. \n",
    "# spelling\n",
    "\n",
    "\n",
    "clear_output()\n",
    "british_eng = defaultdict(str, british_eng)\n",
    "# british_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling mistakes are done manually when we check the output of stuffs and hence depends on my expertize in English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6513cc9bb1e44a479a70ad458048d9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spelling_mistakes = {\n",
    "    \"bahaviour\": \"behavior\",\n",
    "    \"excercise\": \"exercise\",\n",
    "    \"supprise\": \"surprise\",\n",
    "    \"suprise\": \"surprise\",\n",
    "    \"appologise\": \"apologize\",\n",
    "    \"appologize\": \"apologize\",\n",
    "    'excersise': \"exercise\",\n",
    "    'oterwise': \"otherwise\",\n",
    "    \"frnachise\": \"franchise\",\n",
    "    \"fulfullment\": \"fulfillment\",\n",
    "    'usuallu': \"usually\",\n",
    "    'specfically': \"specifically\",\n",
    "    'espically': \"especially\",\n",
    "    \"talll\": \"tall\",\n",
    "    'usally': \"usually\",\n",
    "    'ususally': \"unusually\",\n",
    "    'adventually': 'eventually',\n",
    "    'oscialltor': 'oscillator',\n",
    "    'xcellerator': 'accelerator',\n",
    "    'reccollecting': 'recollecting',\n",
    "    'osciallator': 'oscillator',\n",
    "    'unballance': 'unbalance',\n",
    "    'congroller': 'controller',\n",
    "    'weeeeelllllll': 'well',\n",
    "    'killig': 'killing',\n",
    "    'oscilliscope': \"oscilloscope\",\n",
    "    \"ussually\": \"usually\",\n",
    "    'knoew': 'knew',\n",
    "    \"hense\": \"hence\",\n",
    "    \n",
    "}\n",
    "\n",
    "spelling_mistakes.update(brit_to_amer(r'[a-z]+lll[a-z]+$', 'lll', 'll'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe rumor was basically everywhere in Toronto based on reports\\nthat Keenan has told both San Jose and Philadelphia that he\\nwas no longer interested in pursuing further negotiations with\\neither team. \\n\\nThe Ranger announcement is supposed to happen tomorrow supposedly.\\n\\nThe Rangers have so many veterans that they had to get a coach\\nwith \"weight\" and a proven record...and whom they know Messier respects.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_xs[251].replace(\"rumour\", \"rumor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace spelling mistakes before british to american. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2332e3d2bb5a0777e9aeaf29763188d62a394303965da046a80937952abed10b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dg1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
